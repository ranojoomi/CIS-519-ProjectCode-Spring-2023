{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ZBeMZv4EhQZIPyou09l2fUbfIVn-gLW9","timestamp":1682443570839}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"6Fam410cQUPp","executionInfo":{"status":"ok","timestamp":1682444083549,"user_tz":240,"elapsed":310,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow\n","from gensim.models.word2vec import Word2Vec\n","from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n","from keras.preprocessing.text import Tokenizer\n","# from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import PorterStemmer\n","from sklearn import preprocessing\n","import gensim\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n","from tensorflow.keras.models import Sequential\n","import nltk"]},{"cell_type":"code","source":["def preprocess_text(sen):\n","    # Remove punctuations and numbers\n","    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n","\n","    # Single character removal\n","    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n","\n","    # Removing multiple spaces\n","    sentence = re.sub(r'\\s+', ' ', sentence)\n","    \n","    stops = stopwords.words('english')\n","    #print(stops)\n","    porter = PorterStemmer()\n","    for word in sentence.split():\n","        if word in stops:\n","            sentence = sentence.replace(word, '')\n","        sentence = sentence.replace(word, porter.stem(word))\n","    return sentence.lower()"],"metadata":{"id":"q5vI_KKRQhi4","executionInfo":{"status":"ok","timestamp":1682444084031,"user_tz":240,"elapsed":3,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","df = pd.read_csv('../content/drive/MyDrive/Reviews.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfnuWQq1QhlW","executionInfo":{"status":"ok","timestamp":1682444096248,"user_tz":240,"elapsed":12220,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"34abf28a-63fe-4964-9837-3d79d65b4b40"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["\n","df = df[['Text','Score']]\n","df['Score'].isnull().sum()\n","df['Score'].isnull().sum()\n","df.drop_duplicates(subset=['Text','Score'],keep='first',inplace=True)\n","\n","def set_sent(score):\n","    if score <= 3:\n","        return 0\n","    else:\n","        return 1\n","\n","df['sentiment']=df['Score'].apply(set_sent)\n","df = df[['Text','sentiment']]\n","\n","# Separate into three sentiment groups\n","df_0 = df[df['sentiment'] == 0]\n","df_1 = df[df['sentiment'] == 1]\n","\n","n = 25000\n","\n","df_0 = df_0.sample(n=n, random_state=42, replace=False)\n","df_1 = df_1.sample(n=n, random_state=42, replace=False)\n","# print(\"df_neg1: \", df_neg1)\n","# print(\"df_0: \", df_0)\n","# print(\"df_1: \", df_1)\n","\n","sub_df = pd.concat([df_0, df_1], axis=0)\n","X = sub_df['Text']\n","y = sub_df['sentiment']"],"metadata":{"id":"Do5l28aCRafi","executionInfo":{"status":"ok","timestamp":1682444098807,"user_tz":240,"elapsed":2561,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["mes = []\n","for i in X:\n","    mes.append(i.split())\n","print(mes[:2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ptIxHOUKQhn8","executionInfo":{"status":"ok","timestamp":1682444099781,"user_tz":240,"elapsed":976,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"37b46cd4-9b72-41ed-8b1f-d9459f6a7bd1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[['Some', 'neighborhood', 'cat', 'uses', 'our', 'backyard', 'as', 'her', 'sandbox', 'every', 'night.', 'We', 'sprinkled', 'Critter', 'Ridder', 'heavily', 'around', 'the', \"cat's\", 'favorite', 'spots.', 'Not', 'only', 'did', 'the', 'cat', 'come', 'back,', 'she', 'pooped', 'on', 'one', 'of', 'the', 'treated', 'spots', 'again.'], ['Very', 'nice', 'packaging', 'presentation', 'and', 'very', 'nice', 'assortment', 'of', 'candy.', 'The', 'problem', 'with', 'the', 'candy', 'was', 'that', 'the', 'Violet', 'candy', 'that', 'was', 'included', 'in', 'the', 'box', 'gave', 'almost', 'all', 'of', 'the', 'other', 'candy', 'in', 'the', 'box', 'a', 'flowery', 'flavor.']]\n"]}]},{"cell_type":"code","source":["w2v_model = Word2Vec(mes, vector_size=100, window=6, min_count=1, workers=16)\n","print(w2v_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhChcFxvQhqO","executionInfo":{"status":"ok","timestamp":1682444147156,"user_tz":240,"elapsed":47379,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"32d31d66-3698-4b8f-efff-f22034939ee6"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Word2Vec<vocab=162214, vector_size=100, alpha=0.025>\n"]}]},{"cell_type":"code","source":["# Preprocess the text data\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stop_words = set(stopwords.words('english'))\n","def preprocess(text):\n","    text = text.lower()\n","    text = ''.join([word for word in text if word not in string.punctuation])\n","    tokens = word_tokenize(text)\n","    tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(tokens)\n","\n","X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2)\n","\n","X_train = X_train.apply(preprocess)\n","X_test = X_test.apply(preprocess)\n","\n","max_length = 100\n","# Tokenize the text data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n","X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n","vocab_size = len(tokenizer.word_index) + 1\n","# Create a weight matrix for the embedding layer\n","embedding_matrix = np.zeros((vocab_size, 100))\n","for word, i in tokenizer.word_index.items():\n","    if word in w2v_model.wv:\n","        embedding_matrix[i] = w2v_model.wv[word]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uugRQEMFQhtf","executionInfo":{"status":"ok","timestamp":1682444173541,"user_tz":240,"elapsed":26395,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"7898d99b-7497-4be1-d992-5a1d2ad5dc63"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["# Define the CNN model\n","max_length = 300\n","model = Sequential()\n","model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(5))\n","model.add(Conv1D(256, 5, activation='relu'))\n","model.add(MaxPooling1D(5))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(5))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":623},"id":"TS39qOdJQr2v","executionInfo":{"status":"error","timestamp":1682444173829,"user_tz":240,"elapsed":291,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"40bc39d7-7f69-4660-9bbd-4e75963346e5"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-e952ef1d683f>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 300), found shape=(32, 100)\n"]}]}]}