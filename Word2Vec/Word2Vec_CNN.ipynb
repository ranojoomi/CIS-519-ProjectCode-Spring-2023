{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"m_wa1M9RtRhL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682442040145,"user_tz":240,"elapsed":32231,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"a75dd780-8296-4097-d588-e667d65f846c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Read in the data and clean up column names\n","import gensim\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n","from tensorflow.keras.models import Sequential\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["df = pd.read_csv('../content/drive/MyDrive/Reviews.csv')\n","\n","df = df[['Text','Score']]\n","df['Score'].isnull().sum()\n","df['Score'].isnull().sum()\n","df.drop_duplicates(subset=['Text','Score'],keep='first',inplace=True)\n","\n","def set_sent(score):\n","    if score<=2:\n","        return 0\n","    elif score==3:\n","        return 1\n","    else:\n","        return 2\n","\n","df['sentiment']=df['Score'].apply(set_sent)\n","df = df[['Text','sentiment']]\n","\n","# Separate into three sentiment groups\n","df_neg1 = df[df['sentiment']==0]\n","df_0 = df[df['sentiment']==1]\n","df_1 = df[df['sentiment']==2]\n","\n","n = 15000\n","\n","df_neg1 = df_neg1.sample(n=n, random_state=42, replace=False)\n","df_0 = df_0.sample(n=n, random_state=42, replace=False)\n","df_1 = df_1.sample(n=n, random_state=42, replace=False)\n","# print(\"df_neg1: \", df_neg1)\n","# print(\"df_0: \", df_0)\n","# print(\"df_1: \", df_1)\n","\n","sub_df = pd.concat([df_neg1, df_0, df_1], axis=0)\n","X = sub_df['Text']\n","\n","# Tokenize using gensim\n","# X = X.apply(lambda x: gensim.utils.simple_preprocess(x))\n","y = sub_df['sentiment']"],"metadata":{"id":"HF0EFGj1uz2l","executionInfo":{"status":"ok","timestamp":1682442051613,"user_tz":240,"elapsed":11472,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2)\n"],"metadata":{"id":"-1Lo51dRu2c3","executionInfo":{"status":"ok","timestamp":1682442051613,"user_tz":240,"elapsed":4,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Preprocess the text data\n","stop_words = set(stopwords.words('english'))\n","def preprocess(text):\n","    text = text.lower()\n","    text = ''.join([word for word in text if word not in string.punctuation])\n","    tokens = word_tokenize(text)\n","    tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(tokens)\n","\n","X_train = X_train.apply(preprocess)\n","X_test = X_test.apply(preprocess)\n","\n","# Tokenize the text data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","\n","\n","vocab_size = len(tokenizer.word_index) + 1"],"metadata":{"id":"6wrsXElDIEHc","executionInfo":{"status":"ok","timestamp":1682442082082,"user_tz":240,"elapsed":30472,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Pad the sequences to a fixed length\n","max_length = 100\n","print(X_train[0])\n","X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n","X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n","\n","print(X_train.shape)\n","print(X_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G0R827hiNnX_","executionInfo":{"status":"ok","timestamp":1682442082082,"user_tz":240,"elapsed":14,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"257472d5-fbe6-4c66-8038-243ad09d6da4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[497, 32, 8032, 14093, 372, 9, 78, 133, 1525, 292, 9, 712, 79, 63, 1727, 1195, 2778, 2472, 928, 977, 14, 9, 611, 32, 8032, 301, 372, 789, 9, 266, 9, 1040, 731, 9, 21, 860, 8, 60, 1270, 54, 84, 25, 100, 9371, 199, 372, 789, 8394, 834, 311, 7, 849, 82, 404]\n","(36000, 100)\n","[  497    32  8032 14093   372     9    78   133  1525   292     9   712\n","    79    63  1727  1195  2778  2472   928   977    14     9   611    32\n","  8032   301   372   789     9   266     9  1040   731     9    21   860\n","     8    60  1270    54    84    25   100  9371   199   372   789  8394\n","   834   311     7   849    82   404     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0]\n"]}]},{"cell_type":"code","source":["sen = []\n","for i in X_train:\n","    sen.append(i.split())\n","print(sen[:2])"],"metadata":{"id":"xqlPe7FrWwdt","colab":{"base_uri":"https://localhost:8080/","height":217},"executionInfo":{"status":"error","timestamp":1682442082450,"user_tz":240,"elapsed":380,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"75cd587c-9e13-4b69-bc7f-77399e6b1e69"},"execution_count":6,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e71643b1b945>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'split'"]}]},{"cell_type":"code","source":["# Train the Word2Vec model\n","# sentences = [sentence for sentence in X_train]\n","w2v_model = Word2Vec(sen, window=5, min_count=5, workers=4)\n","\n","# Create a weight matrix for the embedding layer\n","embedding_matrix = np.zeros((vocab_size, 100))\n","for word, i in tokenizer.word_index.items():\n","    if word in w2v_model.wv:\n","        embedding_matrix[i] = w2v_model.wv[word]"],"metadata":{"id":"ieM_h0CBvMgT","executionInfo":{"status":"aborted","timestamp":1682442082451,"user_tz":240,"elapsed":6,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the CNN model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(5))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(MaxPooling1D(5))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"],"metadata":{"id":"VJKv7YuVFEw-","executionInfo":{"status":"aborted","timestamp":1682442082451,"user_tz":240,"elapsed":6,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"execution_count":null,"outputs":[]}]}