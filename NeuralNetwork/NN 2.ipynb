{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9870,"status":"ok","timestamp":1682524018827,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"},"user_tz":240},"id":"-c4nDilToZJg","outputId":"d56ec92f-0616-4a80-ace9-782227a05152"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# Ignore  the warnings\n","import warnings\n","warnings.filterwarnings('always')\n","warnings.filterwarnings('ignore')\n","\n","# data visualisation and manipulation\n","import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.transforms import ToTensor\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.stem.porter import PorterStemmer\n","stemmer = PorterStemmer()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28021,"status":"ok","timestamp":1682524046841,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"},"user_tz":240},"id":"aenRrPiGorsi","outputId":"6b654fb1-457a-4d6e-ab8f-3dc9dc91f3bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UxfZf_zsotTH","executionInfo":{"status":"ok","timestamp":1682524055545,"user_tz":240,"elapsed":8707,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"outputs":[],"source":["df = pd.read_csv('../content/drive/MyDrive/Reviews.csv')\n","\n","df = df[['Text','Score']]\n","df['Score'].isnull().sum()\n","df['Score'].isnull().sum()\n","df.drop_duplicates(subset=['Text','Score'],keep='first',inplace=True)\n","\n","def set_sent(score):\n","    if score<=2:\n","        return -1\n","    elif score==3:\n","        return 0\n","    else:\n","        return 1\n","\n","df['sentiment']=df['Score'].apply(set_sent)\n","df = df[['Text','sentiment']]\n","\n","# Separate into three sentiment groups\n","df_neg1 = df[df['sentiment']==-1]\n","df_0 = df[df['sentiment']==-0]\n","df_1 = df[df['sentiment']==1]\n","\n","n = 5000\n","\n","df_neg1 = df_neg1.sample(n=n, random_state=42, replace=False)\n","df_0 = df_0.sample(n=n, random_state=42, replace=False)\n","df_1 = df_1.sample(n=10000, random_state=42, replace=False)\n","# print(\"df_neg1: \", df_neg1)\n","# print(\"df_0: \", df_0)\n","# print(\"df_1: \", df_1)\n","\n","sub_df = pd.concat([df_neg1, df_0, df_1], axis=0)\n","X = sub_df['Text']\n","y = sub_df['sentiment']\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zy8-1uW8phiH","executionInfo":{"status":"ok","timestamp":1682524055546,"user_tz":240,"elapsed":13,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"outputs":[],"source":["def tokenize(sentence):\n","    return nltk.word_tokenize(sentence)\n","\n","def stem(word):\n","    return stemmer.stem(word.lower())\n","\n","def bag_of_words(tokenized_sentence, words):\n","\n","    # stem each word\n","    sentence_words = [stem(word) for word in tokenized_sentence]\n","    # initialize bag with 0 for each word\n","    bag = np.zeros(len(words), dtype=np.float32)\n","    for idx, w in enumerate(words):\n","        if w in sentence_words: \n","            bag[idx] = 1\n","\n","    return bag"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"1vkefvC-sonc","executionInfo":{"status":"ok","timestamp":1682524106676,"user_tz":240,"elapsed":51143,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"outputs":[],"source":["sentiments = []\n","all_words = []\n","xy = []\n","\n","for sentiment in y:\n","    sentiments.append(sentiment)\n","\n","i = 0\n","\n","for text in X:\n","    w = tokenize(text)\n","    all_words.extend(w)\n","    xy.append((w, sentiments[i]))\n","    i += 1\n","\n","\n","ignore_words = ['?', '!', '.', ',']\n","\n","all_words = [stem(w) for w in all_words if w not in ignore_words]\n","all_words = sorted(set(all_words))\n","sentiments = sorted(set(sentiments))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682524106676,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"},"user_tz":240},"id":"cqhnojY0V93v","outputId":"a2a4a33b-83c6-48fb-83dc-a88d8ff2fd0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["(['I', 'thought', 'this', 'fish', 'oil', 'would', 'be', 'a', 'great', 'supplement', 'for', 'my', 'cats', 'after', 'one', 'of', 'them', 'began', 'to', 'overgroom', 'and', 'created', 'bald', 'patches', 'on', 'her', 'stomach', 'and', 'legs', '.', 'All', 'four', 'of', 'my', 'cats', 'refused', 'any', 'type', 'of', 'food', 'with', 'this', 'fish', 'oil', 'added', '.', 'I', 'tried', 'for', 'many', 'days', 'in', 'many', 'ways', '.', 'Finally', 'switched', 'back', 'from', 'a', 'newer', 'dry', 'food', 'I', 'thought', 'would', 'be', 'better', 'for', 'them', ',', 'back', 'to', 'their', 'old', 'dry', 'food', ',', 'which', 'obviously', 'had', 'more', 'oils', 'in', 'the', 'ingredients', '.', 'Bald', 'cat', 'is', 'getting', 'her', 'fur', 'back', '.'], -1)\n"]}],"source":["# train val and test split\n","train, test = train_test_split(xy, test_size=0.2, random_state=42, shuffle=True)\n","\n","test, val = train_test_split(test, test_size=0.5, random_state=42)\n","\n","print(test[0])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"BGlLvZcLcTCk","executionInfo":{"status":"ok","timestamp":1682526044985,"user_tz":240,"elapsed":1938313,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"outputs":[],"source":["def make_into_bag (df):\n","  X = [] \n","  y = [] \n","\n","  for (pattern_sentence, tag) in df:\n","      bag = bag_of_words(pattern_sentence, all_words)\n","      X.append(bag)\n","\n","      label = sentiments.index(tag)\n","      y.append(label)\n","\n","  X = np.array(X)\n","  y = np.array(y)\n","  return X, y\n","\n","X_train, y_train = make_into_bag(train)\n","X_test, y_test = make_into_bag(test)\n","X_val, y_val = make_into_bag(train)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nkmY9_mKcPZR","executionInfo":{"status":"ok","timestamp":1682526044989,"user_tz":240,"elapsed":9,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}}},"outputs":[],"source":["class MainData(Dataset):\n","    def __init__(self, X_data, y_data):\n","        self.n_samples = len(X_data)\n","        self.x_data = X_data\n","        self.y_data = y_data\n","\n","    def __getitem__(self, index):\n","        return self.x_data[index], self.y_data[index]\n","\n","    def __len__(self):\n","        return self.n_samples"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"-yPojh5Yf_uj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682526244113,"user_tz":240,"elapsed":199132,"user":{"displayName":"Radin Nojoomi","userId":"08257129610895475884"}},"outputId":"66322b7d-fe17-4d3b-adeb-2003e70996ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1:\n","Training Loss: 0.87\n","Validation Loss: 1.431\n","Training Accuracy: 0.593\n","Validation Accuracy: 0.392\n","\n","Epoch 2:\n","Training Loss: 0.638\n","Validation Loss: 2.35\n","Training Accuracy: 0.721\n","Validation Accuracy: 0.384\n","\n","Epoch 3:\n","Training Loss: 0.504\n","Validation Loss: 2.72\n","Training Accuracy: 0.796\n","Validation Accuracy: 0.413\n","\n","Epoch 4:\n","Training Loss: 0.438\n","Validation Loss: 3.741\n","Training Accuracy: 0.836\n","Validation Accuracy: 0.401\n","\n","Epoch 5:\n","Training Loss: 0.347\n","Validation Loss: 4.146\n","Training Accuracy: 0.873\n","Validation Accuracy: 0.356\n","\n","Epoch 6:\n","Training Loss: 0.303\n","Validation Loss: 2.923\n","Training Accuracy: 0.897\n","Validation Accuracy: 0.371\n","\n","Epoch 7:\n","Training Loss: 0.273\n","Validation Loss: 6.133\n","Training Accuracy: 0.909\n","Validation Accuracy: 0.387\n","\n","Epoch 8:\n","Training Loss: 0.244\n","Validation Loss: 4.191\n","Training Accuracy: 0.923\n","Validation Accuracy: 0.346\n","\n","Epoch 9:\n","Training Loss: 0.224\n","Validation Loss: 8.973\n","Training Accuracy: 0.931\n","Validation Accuracy: 0.397\n","\n","Epoch 10:\n","Training Loss: 0.223\n","Validation Loss: 4.738\n","Training Accuracy: 0.929\n","Validation Accuracy: 0.355\n","\n"]}],"source":["num_epochs = 10\n","batch_size = 32\n","learning_rate = 0.01\n","input_size = len(X_train[0])\n","hidden_size = 64\n","output_size = len(sentiments)\n","\n","train_loader = DataLoader(dataset=MainData(X_train, y_train), batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = DataLoader(dataset=MainData(X_test, y_test), batch_size=batch_size, shuffle=True, num_workers=0)\n","val_loader = DataLoader(dataset=MainData(X_val, y_val), batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class Net(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(Net, self).__init__()\n","        self.l1 = nn.Linear(input_size, 32) \n","        self.l2 = nn.Linear(32, 64) \n","        self.l3 = nn.Linear(64, 256)\n","        self.l4 = nn.Linear(256, 512)\n","        self.l5 = nn.Linear(512, 256)\n","        self.l6 = nn.Linear(256, 64)\n","        self.l7 = nn.Linear(64, 32)\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","        out = self.l1(x)\n","        out = self.relu(out)\n","        out = self.l2(out)\n","        out = self.relu(out)\n","        out = self.l3(out)\n","        out = self.relu(out)\n","        out = self.l4(out)\n","        out = self.relu(out)\n","        out = self.l5(out)\n","        out = self.relu(out)\n","        out = self.l6(out)\n","        out = self.relu(out)\n","        out = self.l7(out)\n","        # no activation and no softmax at the end\n","        return out\n","\n","model = Net(input_size, hidden_size, output_size).to(device)\n","\n","# Loss and optimizer (optimizer can be played around)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_loss, val_loss = [], []\n","train_acc, val_acc = [], []\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","\n","    model.train()\n","    running_loss = 0.\n","    correct, total = 0, 0 \n","\n","    for (words, labels) in train_loader:\n","        words = words.to(device)\n","        labels = labels.to(dtype=torch.long).to(device)\n","        \n","        # Forward pass\n","        outputs = model(words)\n","        # if y would be one-hot, we must apply\n","        # labels = torch.max(labels, 1)[1]\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","            \n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    \n","    train_loss.append(running_loss / len(train_loader))\n","    train_acc.append(correct/total)\n","\n","    model.eval()\n","    running_loss = 0.\n","    correct, total = 0, 0\n","\n","    for (words, labels) in val_loader:\n","        words = words.to(device)\n","        labels = labels.to(dtype=torch.long).to(device)\n","\n","        output = model(words)\n","        loss = criterion(outputs, labels)\n","\n","        running_loss += loss.item()\n","\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    val_loss.append(running_loss / len(val_loader))\n","    val_acc.append(correct/total)\n","        \n","    if (epoch+1) % 100 == 0:\n","        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","\n","    print(f\"Epoch {epoch+1}:\")\n","    print(f\"Training Loss:\", round(train_loss[epoch], 3))\n","    print(f\"Validation Loss:\", round(val_loss[epoch], 3))\n","    print(f\"Training Accuracy:\", round(train_acc[epoch], 3))\n","    print(f\"Validation Accuracy:\", round(val_acc[epoch], 3))\n","    print()\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"14AVfPjKZvra3yDrnQELSnYFuTDLjv7zF","timestamp":1682373151225}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}